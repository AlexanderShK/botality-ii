export default {
  dashboard: 'Dashboard',
  configuration: 'Configuration',
  model_manager: 'Model manager',
  chat: 'Chat',
  page404: 'This page does not exist.',
  goBack: 'Bring me back Home',
  bot: 'Bot',
  status: 'Status',
  flags: 'Flags',
  started: 'started',
  stopped: 'stopped',
  can_join_groups: 'this bot can join groups',
  cannot_join_groups: 'this bot cannot join groups, change configuration in BotFather',
  can_read_all: 'this bot can read all group messages',
  cannot_read_all: 'this bot cannot read all group messages and it is only activated via commands and replies',
  access_mode: 'Access restriction mode',
  main_active_modules: 'Active modules (init time)',
  uptime: 'Uptime',
  seconds: 'seconds',
  total_messages: 'Total messages',
  total: 'TOTAL',
  consumed: 'CONSUMED',
  process: 'PROCESS',
  cache: 'Cache',
  cache_size: 'Estimated initial size',
  processing: 'processing',
  chat_offline: 'Please start the bot to chat with it.',
  mmanager_offline: 'Please start the bot to manage models.',
  empty_message: 'Type your message',
  recommended_models: 'Recommended Models',
  installed_models: 'Installed Models',
  model_manager_header: 'Pretrained Models',
  install_model: 'Install custom model',
  name: 'Name',
  voice: 'Voice',
  repo: 'Repo',
  path: 'Path',
  size: 'Size (GB)',
  are_you_sure: 'Are you sure?',
  confirm_uninstall: 'Please confirm that you want to delete the model in the terminal',
  install: 'Install',
  uninstall: 'Uninstall',
  model_type: 'Model type',
  quant: 'Quantization',
  base_voice: 'Base voice',
  confirm_install: 'Please confirm the installation in the terminal!',
  no_base_voice: 'Base voice is required for voice-to-voice models',
  wrong_repo_format: 'Repo path must end have a shash in it',
  model_installed: 'âœ… Model installed!',
  hf_repo: 'HuggingFace Repo',
  repo_path: 'Repo path',
  install_path: 'Install path',
  model_filename: 'Model filename',
  // config
  adminlist: 'A list of ids of bot administrators that have access to admin commands',
  whitelist: 'A list of chat ids and user ids that are allowed to trigger the bot',
  blacklist: 'A list of chat ids and user ids that are not allowed to trigger the bot',
  ignore_mode: 'A mode in which acces-lists are checked. In whitelist mode, only users in whitelist can access the bot, in blacklist, users in blacklist cannot access the bot, when both modes are active, only whitelisted chats are allowed and blacklisted users in them are ignored.',
  active_modules: 'Specify which bot modules do you want to use',
  threaded_initialization: 'When on, each module (and other components) are loaded in parallel',
  tts_path: 'Path to a directory coqui tts models, currently only VITS models were tested.',
  tts_voices: 'A configuration list with each voice for coqui tts',
  tts_mode: 'In local mode, tts is used in the bot process, in remote, it makes an API request to a remote-server and recieves a file, in localhttp mode, it only acts as remote but only sends the path instead of the whole file.',
  tts_replacements: 'An array of strings that will be replaced before passing to the TTS provider, if something does not sound right, try to fix it with a replacement!',
  tts_credits: 'A text that is passed to a help command that credits the people who trained and published the tts models',
  tts_ffmpeg_path: 'A path to a ffmpeg binary that will be used to convert audio to voice messages',
  tts_enable_backends: 'A list of different text-to-speech backends, each with its own voices.',
  tts_list_system_voices: 'Some voices are needed only as a base for speech-to-speech transformation. When this is off, only non-system voices are listed in help command',
  tts_so_vits_svc_4_0_code_path: 'A path to a directory with code of so-vits-svc version 4.0',
  tts_so_vits_svc_4_1_code_path: 'A path to a directory with code of so-vits-svc version 4.1',
  tts_so_vits_svc_voices: 'A configuration list with each voice for so-vits-svc',
  tts_queue_size_per_user: 'Command request cap for TTS',
  tts_host: 'A URL for remote TTS API server (servers/tts_server.py)',
  sd_host: 'A URL for AUTOMATIC1111/stable-diffusion-webui API server',
  sd_max_steps: 'Maximum denoising steps',
  sd_max_resolution: '',
  sd_available_samplers: 'Which samplers are allowed',
  sd_extra_prompt: 'Prompt wrapper, used with /tti and /iti commands, to run SD without prompt wrapper, use /ttiraw /itiraw',
  sd_extra_negative_prompt: 'Negative prompt wrapper',
  sd_default_sampler: '',
  sd_default_n_iter: 'How many images should it generate',
  sd_default_width: '',
  sd_default_height: '',
  sd_default_tti_steps: '',
  sd_default_tti_cfg_scale: '',
  sd_default_iti_cfg_scale: '',
  sd_default_iti_steps: '',
  sd_default_iti_denoising_strength: 'How much denoising should be applied for image-to-image operations',
  sd_default_iti_sampler: '',
  sd_launch_process_automatically: 'Run webui subprocess on demand',
  sd_launch_command: 'command to run webui',
  sd_launch_dir: 'Path to Stable Diffusion webui directory (folder)',
  sd_launch_waittime: 'Time in seconds to wait after starting the subprocess',
  sd_lora_custom_activations: 'A list of custom keywords to trigger loras in format "custom_keyword": "<lora:name:0.8> trigger_keywords"',
  sd_only_admins_can_change_models: 'Allow only bot admins to change the models',
  sd_queue_size_per_user: 'How many images can user request to generate before hitting the queue limit',
  llm_host: 'A URL of an API server such as oobabooga/text-generation-webui API, kobold.cpp API or llama.cpp server',
  llm_queue_size_per_user: 'LLM request queue limit',
  llm_backend: '',
  llm_python_model_type: '',
  llm_paths: 'A configuration key-value list of model components such as weights. Usually you need to change only ones that are related to a chosen LLM backend.',
  llm_character: 'A character file, that contains prompt formatter for specific model and character description for RP',
  llm_history_grouping: 'Can be "user" to store history with each user separately or "chat" to store group chat history with all users in that chat',
  llm_max_history_items: 'How many messages are used for a sliding-window history in conversations',
  llm_generation_cfg_override: '',
  llm_assistant_cfg_override: '',
  llm_assistant_chronicler: 'llm_assistant_chronicler is using alpaca chronicler format for Q/A and custom formats specified in character files, raw format is only needed for mlc backend',
  llm_assistant_use_in_chat_mode: 'Instead of forming a conversation history with a sliding window, always use prompt-format specified in character file for instruct models',
  llm_assistant_add_reply_context: 'Allows replying to Q/A messages to add 1-step memory',
  llm_force_assistant_for_unsupported_models: 'For non-instruct models, allow assistant usage (this will be reworked)',
  llm_max_tokens: 'Max. new tokens in chat (sliding-window) mode',
  llm_max_assistant_tokens: 'Max new tokens in assistant (/ask) mode',
  llm_lcpp_gpu_layers: 'Llama.cpp GPU layer count',
  llm_lcpp_max_context_size: 'Llama.cpp context limit',
  llm_remote_launch_process_automatically: 'Launch remote API server on demand in a subprocess',
  llm_remote_launch_command: 'Terminal command to lauch the server',
  llm_remote_launch_dir: 'A path to a directory when the command will be executed',
  llm_remote_launch_waittime: 'How many seconds to wait after launching the LLM API server subprocess',
  llm_remote_model_name: 'Model filename that will be used in a remote backend (if it supports this)',
  stt_backend: 'Speech-to-text engine',
  stt_model_path_or_name: 'Path or name of a speech-to-text model',
  stt_autoreply_mode: '',
  stt_autoreply_voice: '',
  stt_queue_size_per_user: '',
  tta_queue_size_per_user: '',
  tta_device: 'Text-To-Audio processing device',
  tta_music_model: 'Path or name of a TTA music model',
  tta_sfx_model: 'Path or name of a TTA sfx model',
  tta_duration: 'Duration of generated audio',
  python_command: 'Python command (currently used to run so vits svc audio-to-audio process)',
  mm_preload_models_on_start: 'If on, models will be pre-loaded when the bot is laucnhed otherwise, loading starts on-demand',
  mm_ram_cached_model_count_limit: 'How many models can be loaded in RAM at the same time',
  mm_vram_cached_model_count_limit: 'How many models can be loaded in VRAM at the same time',
  mm_management_policy: 'In COUNT mode only after hitting a limit model will be unloaded, in MEMORY mode, when a there is almost no memory left, a model will be unloaded (but there is no estimation of memory footprint for a model that will be loaded instaed)',
  mm_unload_order_policy: 'Which model should be unloaded first when a limit is reached',
  mm_autounload_after_seconds: 'After how many seconds of inactivity model should be marked for unload',
  sys_webui_host: '',
  sys_api_host: '',
  sys_request_timeout: 'Request timeout, affects browser requests when you want to chat with the model in webui',
  sys_api_log_level: 'Useful for debugging'
}
